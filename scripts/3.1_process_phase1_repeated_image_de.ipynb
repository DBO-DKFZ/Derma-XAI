{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28382573-4cdc-4688-bdcb-2e8ad6d54aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kti01/miniconda3/envs/artifact_detection/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import copy\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import jaccard_score\n",
    "from matplotlib.path import Path\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from skimage import draw\n",
    "from skimage import io\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "plt.rcParams[\"figure.figsize\"] = (15,8)\n",
    "from skimage.io import imread, imshow, imsave\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.draw import polygon\n",
    "from skimage import draw\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "\n",
    "mappings = {\n",
    "\t'Thick reticular or branched lines': 'TRBL',\n",
    "\t'Dicke retikuläre oder verzweigte Linien': 'TRBL',\n",
    "\n",
    "\t'Eccentrically located structureless area (any colour except skin colour, white and grey)': 'ESA',\n",
    "\t'Exzentrisch gelegenes, strukturloses Areal jeglicher Farbe, außer hautfarben, weiß und grau': 'ESA',\n",
    "\n",
    "\t'Grey patterns': 'GP',\n",
    "\t'Graue Muster': 'GP',\n",
    "\n",
    "\t'Polymorphous vessels': 'PV',\n",
    "\t'Polymorphe Gefäße': 'PV',\n",
    "\n",
    "\t'Pseudopods or radial lines at the lesion margin that do not occupy the entire lesional circumference': 'PRL',\n",
    "\t'Pseudopodien oder radiale Linien am Läsionsrand, die nicht den gesamten Läsionsumfang einnehmen': 'PRL',\n",
    "\n",
    "\t'Black dots or globules in the periphery of the lesion': 'BDG',\n",
    "\t'Schwarze Punkte oder Schollen in der Läsionsperipherie': 'BDG',\n",
    "\n",
    "\t'White lines or white structureless area': 'WLSA',\n",
    "\t'Weiße Linien oder weißes strukturloses Areal': 'WLSA',\n",
    "\n",
    "\t'Parallel lines on ridges (acral lesions only)': 'PLR',\n",
    "\t'Parallele Linien auf den Leisten (nur akrale Läsionen)': 'PLR',\n",
    "\n",
    "\t'Pigmentation extends beyond the area of the scar (only after excision)': 'PES',\n",
    "\t'Pigmentierung überschreitet Narbenareal (nur nach Entfernung)': 'PES',\n",
    "\n",
    "\t'Pigmentation invades the openings of hair follicles (facial lesions)': 'PIF',\n",
    "\t'Pigmentierung überschreitet Follikelöffnung (Gesichtsläsionen)': 'PIF',\n",
    "\n",
    "\t'Only one pattern and only one colour': 'OPC',\n",
    "\t'Nur ein Muster und nur eine Farbe': 'OPC',\n",
    "\n",
    "\t'Symmetrical combination of patterns and;or colours': 'SPC',\n",
    "\t'Symmetrische Kombination von Mustern und;oder Farben': 'SPC',\n",
    "\n",
    "\t'Monomorphic vascular pattern': 'MVP',\n",
    "\t'Monomorphes Gefäßmuster': 'MVP',\n",
    "\n",
    "\t'Pseudopods or radial lines at the lesional margin involving the entire lesional circumference': 'PRLC',\n",
    "\t'Pseudopodien oder radiale Linien am Läsionsrand über den gesamten Läsionsumfang': 'PRLC',\n",
    "\n",
    "\t'Parallel lines in the furrows (acral lesions only)': 'PLF',\n",
    "\t'Parallele Linien in den Furchen (nur akrale Läsionen)': 'PLF',\n",
    "\n",
    "\t'Pigmentation does not extend beyond the area of the scar (only after excision)': 'PDES',\n",
    "\t'Pigmentierung überschreitet Narbe nicht (nur nach Entfernung)': 'PDES',\n",
    "\n",
    "\t'Asymmetric combination of multiple patterns and;or colours in the absence of other melanoma criteria': 'APC',\n",
    "\t'Asymmetrische Kombination mehrerer Muster und;oder Farben ohne weitere Melanomkriterien': 'APC',\n",
    "\n",
    "\t'Melanoma simulator': 'MS',\n",
    "\t'Melanomsimulator': 'MS',\n",
    "    \n",
    "\t'Please select at least one explanation': 'XX',\n",
    "\t'Bitte wählen Sie mindestens eine Erklärung aus.': 'XX',\n",
    "    'Andere (bitte angeben)': 'XX'\n",
    "}\n",
    "\n",
    "char_class_labels = ['TRBL', 'ESA', 'BDG', 'GP', 'PV', 'PRL', 'WLSA', 'PES', 'PIF', 'OPC', 'SPC', 'MVP', 'PRLC', 'PLF', 'PDES', 'APC', 'MS']\n",
    "mel_class_labels = ['TRBL', 'ESA', 'BDG', 'GP', 'PV', 'PRL', 'WLSA', 'PES', 'PIF', 'PLR']\n",
    "nv_class_labels = ['OPC', 'SPC', 'MVP', 'PRLC', 'PLF', 'PDES', 'APC', 'MS']\n",
    "\n",
    "\n",
    "def process_annotation(polygons):\n",
    "    # Sometimes the polygon label was defined as bounding box (human error),\n",
    "    #  the 'points' key is missing in that case, so the following if condition handles that.\n",
    "    if 'points' in polygons['data']:\n",
    "        polygon = np.array(polygons['data']['points'])\n",
    "    else:\n",
    "        botton_left = polygons['data']['min']\n",
    "        top_right = polygons['data']['max']\n",
    "        botton_right = [top_right[0], botton_left[1]]\n",
    "        top_left = [botton_left[0], top_right[1]]\n",
    "        polygon = np.array([botton_left, botton_right, top_right, top_left])\n",
    "\n",
    "    polygon[:, 0] *= 600\n",
    "    polygon[:, 1] *= 450\n",
    "    polygon = polygon.astype(int)\n",
    "    \n",
    "    # Different spellings of Explanation, sometimes even the English word was defined\n",
    "    try:\n",
    "        explanation = polygons['children']['Erklärung']['data']['selected']\n",
    "    except:\n",
    "        try:\n",
    "            explanation = polygons['children']['Erklärungen']['data']['selected']\n",
    "        except: \n",
    "            explanation = polygons['children']['Explanation']['data']['selected']\n",
    "                \n",
    "    try:\n",
    "        confidence = polygons['children']['Konfidenz']['data']['selected']\n",
    "    except:\n",
    "        confidence = -1\n",
    "    if confidence == 'Bitte wählen Sie eine Konfidenz.':\n",
    "        confidence = -1\n",
    "    else:\n",
    "        confidence = int(confidence)\n",
    "    \n",
    "    if type(explanation) != list:\n",
    "        explanation = [explanation]\n",
    "    explanation = [mappings[exp] for exp in explanation]\n",
    "    \n",
    "    # XX has been mapped from \"Please choose 1 explanation\" and \"Other Explanation\"\n",
    "    # Since XX is auto selected in every explanation we're dropping XX\n",
    "    # Unless XX is the only explanation, which means no explanation was chosen,\n",
    "    #  renaming it to 'None'\n",
    "    while 'XX' in explanation: explanation.remove('XX')\n",
    "    \n",
    "    if len(explanation) == 0:\n",
    "        explanation.append('None')\n",
    "    \n",
    "    return polygon, explanation, confidence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce7c8e8",
   "metadata": {},
   "source": [
    "# Process metadata files individually in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc44fc2c-a155-48a8-b589-e2f97cb9019a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metadata for g80\n",
      "Saved metadata for g50\n",
      "Saved metadata for g40\n",
      "Saved metadata for g49\n",
      "Saved metadata for g55\n",
      "Saved metadata for g77\n",
      "Saved metadata for g3\n",
      "Saved metadata for g42\n",
      "Saved metadata for g60\n",
      "Saved metadata for g9\n",
      "Saved metadata for g44\n",
      "Saved metadata for g31\n",
      "Saved metadata for g43\n",
      "Saved metadata for g53\n",
      "Saved metadata for g6\n",
      "Saved metadata for g22\n",
      "Saved metadata for g71\n",
      "Saved metadata for g39\n",
      "Saved metadata for g69\n",
      "Saved metadata for g61\n",
      "Saved metadata for g51\n",
      "Saved metadata for g26\n",
      "Saved metadata for g28\n",
      "Saved metadata for g72\n",
      "Saved metadata for g79\n",
      "Saved metadata for g15\n",
      "Saved metadata for g78\n",
      "Saved metadata for g24\n",
      "Saved metadata for g5\n",
      "Saved metadata for g35\n",
      "Saved metadata for g11\n",
      "Saved metadata for g29\n",
      "Saved metadata for g48\n",
      "Saved metadata for g19\n",
      "Saved metadata for g10\n",
      "Saved metadata for g30\n",
      "Saved metadata for g32\n",
      "Saved metadata for g36\n",
      "Saved metadata for g14\n",
      "Saved metadata for g85\n",
      "Saved metadata for g76\n",
      "Saved metadata for g82\n",
      "Saved metadata for g16\n",
      "Saved metadata for g23\n",
      "Saved metadata for g34\n",
      "Saved metadata for g73\n",
      "Saved metadata for g27\n",
      "Saved metadata for g65\n",
      "Saved metadata for g84\n",
      "Saved metadata for g38\n",
      "Saved metadata for g37\n",
      "Saved metadata for g57\n",
      "Saved metadata for g21\n",
      "Saved metadata for g66\n",
      "Saved metadata for g33\n",
      "Saved metadata for g25\n",
      "Saved metadata for g75\n"
     ]
    }
   ],
   "source": [
    "#ham = pd.read_csv(\"/home/kti01/Documents/My Files/Data/HAM10000/HAM10000/metadata\")\n",
    "#ham = ham[ham.dx.isin(['mel', 'nv'])]\n",
    "#ham = ham[ham.dx_type=='histo']\n",
    "\n",
    "ham = pd.read_csv(\"/home/kti01/Documents/My Files/Projects/Overlap/data/metadata_testset.csv\").dropna(how='all')\n",
    "ham['mask'] = ham['mask'].astype(int)\n",
    "\n",
    "# CHANGE THIS DIR\n",
    "phase1_dir = \"/home/kti01/Documents/My Files/Projects/Overlap/data/phase1/repeated_image_de\"\n",
    "participants = os.listdir(phase1_dir)\n",
    "participants = [participant for participant in participants if not (participant.endswith('csv') or participant.endswith('pkl'))]\n",
    "participants = [x for x in participants if not x.endswith('.pkl')]\n",
    "confidences = {}\n",
    "repeating_img = {}\n",
    "\n",
    "for participant in participants:\n",
    "    data_dir = os.path.join(phase1_dir, participant)\n",
    "    files = []\n",
    "    for file in ['train.json', 'validation.json', 'test.json']:\n",
    "\n",
    "        data_file = os.path.join(data_dir, file)\n",
    "\n",
    "        with open(os.path.join(data_file)) as f:\n",
    "            d = json.load(f)\n",
    "            annotations = dict(d)['labels']\n",
    "\n",
    "        # Store labels in a dict with image names as keys and labels as values\n",
    "        labels = {annotation['dataId']: annotation['annotations'] for annotation in annotations}\n",
    "\n",
    "\n",
    "        image_list = []\n",
    "        polygons_list = []\n",
    "        explanations_list = []\n",
    "        confidence_list = []\n",
    "        prediction_list = []\n",
    "        for k, v in labels.items():\n",
    "\n",
    "            if len(v['Melanom']) != 0:\n",
    "                for polygons in v['Melanom']:\n",
    "\n",
    "                    polygon, explanation, confidence = process_annotation(polygons)\n",
    "\n",
    "                    image_list.append(k)\n",
    "                    polygons_list.append(polygon)\n",
    "                    explanations_list.append(explanation)\n",
    "                    confidence_list.append(confidence)\n",
    "                    prediction_list.append(1)\n",
    "            elif len(v['Nävus (belassen)']) != 0:\n",
    "                for polygons in v['Nävus (belassen)']:\n",
    "                    polygon, explanation, confidence = process_annotation(polygons)\n",
    "\n",
    "                    image_list.append(k)\n",
    "                    polygons_list.append(polygon)\n",
    "                    explanations_list.append(explanation)\n",
    "                    confidence_list.append(confidence)\n",
    "                    prediction_list.append(0)\n",
    "            elif len(v['Nävus (exzidieren)']) != 0:\n",
    "                for polygons in v['Nävus (exzidieren)']:\n",
    "                    polygon, explanation, confidence = process_annotation(polygons)\n",
    "\n",
    "                    image_list.append(k)\n",
    "                    polygons_list.append(polygon)\n",
    "                    explanations_list.append(explanation)\n",
    "                    confidence_list.append(confidence)\n",
    "                    prediction_list.append(0.5)\n",
    "\n",
    "        labels = pd.DataFrame({'image_id': image_list, 'polygon': polygons_list, 'explanation': explanations_list,\n",
    "                              'confidence': confidence_list, 'prediction': prediction_list})\n",
    "        #labels['explanation'] = labels['explanation'].apply(lambda x: ['Bitte wählen Sie mindestens eine Erklärung aus.'] if len(x)==0 else x)\n",
    "    \n",
    "        \n",
    "        metadata_df = pd.DataFrame(labels.explanation.explode())\n",
    "        metadata_df = pd.merge(labels['image_id'], metadata_df, left_index=True, right_index=True)\n",
    "        metadata_df = pd.get_dummies(metadata_df.explanation)\n",
    "\n",
    "\n",
    "        metadata_df = labels.join(metadata_df)\n",
    "        metadata_df = metadata_df.groupby('image_id').mean()\n",
    "        metadata_df = metadata_df.reset_index()\n",
    "            \n",
    "            \n",
    "        for col in metadata_df.columns.drop(['image_id', 'confidence', 'prediction']):\n",
    "            metadata_df.loc[metadata_df[col]>0, col] = 1 \n",
    "\n",
    "        metadata_df['mask'] = metadata_df['image_id'].apply(lambda x: x.split('.')[0]).astype(np.int64)\n",
    "        #metadata_df.drop('image_id', axis=1, inplace=True)\n",
    "        metadata_df.sort_values('mask', inplace=True)\n",
    "        \n",
    "        \n",
    "        metadata_df = pd.merge(metadata_df, ham[['mask', 'group', 'image_id']].rename({'image_id': 'id'}, axis=1), \n",
    "                               on=['mask'], how='left')\n",
    "\n",
    "        labels['mask'] = labels['image_id'].apply(lambda x: x.split('.')[0]).astype(np.int64)\n",
    "        labels = pd.merge(labels, metadata_df[['mask']], on='mask', how='inner')\n",
    "\n",
    "        # Add polygons as a column\n",
    "        annotations_dict = labels.set_index('image_id', drop=True).groupby(level=0).apply(lambda x: x.to_dict('records')).to_dict()\n",
    "        metadata_df['annotations'] = [annotations_dict[img] for img in metadata_df.image_id]\n",
    "        metadata_df['image'] = metadata_df['mask'].apply(lambda x: str(x)+'.jpg')\n",
    "\n",
    "\n",
    "        # Put each explanation and it's corresponding polygon in individual rows\n",
    "        d = {'image_id': [], 'explanation': [], 'polygon': []}\n",
    "        for idx, row in labels.iterrows():\n",
    "            for exp in row['explanation']:\n",
    "                d['image_id'].append(row['image_id'])\n",
    "                d['explanation'].append(exp)\n",
    "                d['polygon'].append(row['polygon'])\n",
    "        d = pd.DataFrame(d)\n",
    "\n",
    "        final_dict = {'image_id': [], 'explanation': [], 'polygon': []}\n",
    "        # Loop over each image\n",
    "        for image_id in d['image_id'].unique():\n",
    "            # Get sub dataframe consisting of only one image's explanations \n",
    "            sub_d = d[d['image_id'] == image_id]\n",
    "            # Loop over each explanation for the current image\n",
    "            for exp in sub_d['explanation'].unique():\n",
    "                # Select all rows of the dataframe where the explanation is the current explanation\n",
    "                sub_de = sub_d[sub_d['explanation'] == exp]\n",
    "                # Loop over each polygon and append to list\n",
    "                poly_list = []\n",
    "                for poly in sub_de['polygon']:\n",
    "                    poly_list.append(poly)\n",
    "\n",
    "                # Keep appending to lists in a dict so that we can create a dataframe out of this\n",
    "                final_dict['polygon'].append(poly_list)\n",
    "                final_dict['image_id'].append(image_id)\n",
    "                final_dict['explanation'].append(exp)\n",
    "                \n",
    "        final_df = pd.DataFrame(final_dict)\n",
    "        final_df = final_df.pivot_table(values='polygon', index=final_df['image_id'], columns='explanation', aggfunc='first')\n",
    "\n",
    "        final_df.fillna(-1, inplace=True)\n",
    "\n",
    "        # Rename columns\n",
    "        for col in final_df.columns:\n",
    "            final_df.rename(columns={col: col+'_annotation'}, inplace=True)\n",
    "        final_df['participant'] = participant\n",
    "\n",
    "        final_merged_df = pd.merge(metadata_df, final_df, on='image_id', how='inner')\n",
    "        files.append(final_merged_df)\n",
    "        \n",
    "        \n",
    "    final_final_merged_df = pd.concat(files)\n",
    "    final_final_merged_df.reset_index(drop=True, inplace=True)\n",
    "    final_final_merged_df.fillna(-1, inplace=True)\n",
    "    \n",
    "    final_final_merged_df['image_id'] = final_final_merged_df['id']\n",
    "    final_final_merged_df = final_final_merged_df.drop('id', axis=1)\n",
    "    \n",
    "    \n",
    "    final_final_merged_df = pd.merge(final_final_merged_df, ham[['image_id', 'dx']], on='image_id', how='left').drop_duplicates(subset='mask')\n",
    "    final_final_merged_df['benign_malignant'] = final_final_merged_df['dx'].apply(lambda x: 0 if x == 'nv' else 1)\n",
    "    \n",
    "    final_final_merged_df = final_final_merged_df.sort_values('mask').reset_index(drop=True)\n",
    "    \n",
    "    try:\n",
    "        idx = final_final_merged_df[final_final_merged_df['mask'] == 9898].index[0]\n",
    "        final_final_merged_df.at[idx, 'group'] = final_final_merged_df.loc[0, 'group']\n",
    "        final_final_merged_df.at[idx, 'image_id'] = final_final_merged_df.loc[2, 'image_id']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    with open(os.path.join(data_dir, 'metadata.pkl'), 'wb') as fp:\n",
    "        pickle.dump(final_final_merged_df, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('Saved metadata for '+participant)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c48cad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8cb0ca9",
   "metadata": {},
   "source": [
    "# Merge all individual annotator's metadata files into one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "205af78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = os.listdir(phase1_dir)\n",
    "participants = [x for x in participants if not x.endswith('.pkl') and not x.endswith('.csv')]\n",
    "\n",
    "metadata_list = []\n",
    "for participant in participants:\n",
    "    data_dir = os.path.join(phase1_dir, participant)\n",
    "    \n",
    "    metadata = pd.read_pickle(os.path.join(data_dir, 'metadata.pkl'))\n",
    "    metadata['participant'] = participant\n",
    "    metadata_list.append(metadata)\n",
    "metadata_df = pd.concat(metadata_list)\n",
    "metadata_df.fillna(-1, inplace=True)\n",
    "metadata_df = metadata_df[['participant'] + [col for col in metadata_df.columns if col != 'participant' ]]\n",
    "metadata_df['confidence'] = metadata_df['confidence'].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaf0e9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved consolidated metadata file\n"
     ]
    }
   ],
   "source": [
    "metadata_df.to_csv(os.path.join(phase1_dir, 'metadata_phase1_german.csv'), index=False)\n",
    "with open(os.path.join(phase1_dir, 'metadata_phase1_german.pkl'), 'wb') as fp:\n",
    "    pickle.dump(metadata_df, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('Saved consolidated metadata file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b04de41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee24fc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a113205c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b0201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4285c2b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8435a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7da1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67018764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43003c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c75a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab68669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
